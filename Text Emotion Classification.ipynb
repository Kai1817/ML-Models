{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521e158d",
   "metadata": {},
   "source": [
    "## Text Emotion Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64565e1",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2164c4",
   "metadata": {},
   "source": [
    "**Text emotions classification** is the problem of assigning emotion to a text by understanding the context and the emotion behind the text. One real-world example is the keyboard of a phone that recommends the most relevant emoji by understanding the text. So, if you want to learn how to classify the emotions of a text, this article is for you. In this article, I will take you through the task of text emotions classification with Machine Learning using Python.\n",
    "- **Text emotion classification is a task within natural language processing and text classification where the goal is to train a model to identify the emotion conveyed in a text.** To achieve this, we need labeled datasets that associate texts with their corresponding emotions. I discovered an excellent dataset on Kaggle that fits this requirement.\n",
    "- In the following sections, **I will guide you through the process of training a text classification model for text emotion classification using Machine Learning and Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e77185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Emotions\n",
      "0  i can go from feeling so hopeless to so damned...  sadness\n",
      "1   im grabbing a minute to post i feel greedy wrong    anger\n",
      "2  i am ever feeling nostalgic about the fireplac...     love\n",
      "3                               i am feeling grouchy    anger\n",
      "4  ive been feeling a little burdened lately wasn...  sadness\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/asus/OneDrive/Desktop/ML_Datasets/project/More_Projects/train.txt\", sep=';')\n",
    "data.columns = [\"Text\", \"Emotions\"]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936170ad",
   "metadata": {},
   "source": [
    "**from tensorflow.keras.preprocessing.text import Tokenizer:**\n",
    "- The Tokenizer class is used to convert text into sequences of integers. Each word in the text is assigned a unique integer index based on its frequency in the dataset.\n",
    "\n",
    "**from tensorflow.keras.preprocessing.sequence import pad_sequences:**\n",
    "- The pad_sequences function is used to ensure that all sequences (lists of integers) have the same length. This is often necessary for training neural network models, as they require input of consistent dimensions.\n",
    "\n",
    "**from keras.models import Sequential:**\n",
    "- The Sequential class is a linear stack of layers in Keras. It allows you to build a model layer by layer, which is ideal for simple models with a single input and output.\n",
    "\n",
    "**from keras.layers import Embedding, Flatten, Dense:**\n",
    "- **Embedding:** This layer is used to create dense word embeddings from integer sequences. It converts positive integers (indexes) into dense vectors of fixed size.\n",
    "- **Flatten:** This layer flattens the input, i.e., it converts a multi-dimensional tensor into a single-dimensional tensor. This is useful when transitioning from convolutional layers to dense layers.\n",
    "- **Dense:** This is a fully connected neural network layer. Each neuron in this layer is connected to every neuron in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef76fd4",
   "metadata": {},
   "source": [
    "**As this is a problem of natural language processing, I’ll start by tokenizing the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607dfaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"Text\"].tolist()\n",
    "labels = data[\"Emotions\"].tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e66a0",
   "metadata": {},
   "source": [
    "**Now we will pad the sequence, means all the list of integers should have the same length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f959e",
   "metadata": {},
   "source": [
    "**use the label encoder method to convert the classes from strings to a numerical representation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0568661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfa4bc",
   "metadata": {},
   "source": [
    "We are now going to One-hot encode the labels. One hot encoding refers to the transformation of categorical labels into a binary representation where each label is represented as a vector of all zeros except a single 1. This is necessary because machine learning algorithms work with numerical data. So here is how we can One-hot encode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072d9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfeaf9",
   "metadata": {},
   "source": [
    "**Now we will split the data into training and test sets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82639ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(padded_sequences, \n",
    "                                                one_hot_labels, \n",
    "                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66464ec",
   "metadata": {},
   "source": [
    "**A neural network** is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected units called neurons or nodes, which work together to recognize patterns, learn from data, and make decisions. **Neural networks are a fundamental part of deep learning, a subset of machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e621914",
   "metadata": {},
   "source": [
    "Now let’s define a neural network architecture for our classification problem and use it to train a model to classify emotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cca3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=128))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dense(units=len(one_hot_labels[0]), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e847e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.3841 - loss: 1.5245 - val_accuracy: 0.6878 - val_loss: 0.8970\n",
      "Epoch 2/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.8529 - loss: 0.4711 - val_accuracy: 0.8259 - val_loss: 0.5399\n",
      "Epoch 3/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9816 - loss: 0.0731 - val_accuracy: 0.8178 - val_loss: 0.5847\n",
      "Epoch 4/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.9939 - loss: 0.0249 - val_accuracy: 0.8206 - val_loss: 0.6200\n",
      "Epoch 5/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.9965 - loss: 0.0192 - val_accuracy: 0.8288 - val_loss: 0.6260\n",
      "Epoch 6/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9977 - loss: 0.0110 - val_accuracy: 0.8253 - val_loss: 0.6707\n",
      "Epoch 7/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.9975 - loss: 0.0103 - val_accuracy: 0.8231 - val_loss: 0.6873\n",
      "Epoch 8/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9979 - loss: 0.0113 - val_accuracy: 0.8213 - val_loss: 0.7179\n",
      "Epoch 9/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.9977 - loss: 0.0110 - val_accuracy: 0.8203 - val_loss: 0.7501\n",
      "Epoch 10/10\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.9971 - loss: 0.0111 - val_accuracy: 0.8188 - val_loss: 0.7570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c28cde52d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(xtrain, ytrain, epochs=10, batch_size=32, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ad738",
   "metadata": {},
   "source": [
    "**Optimizer:**\n",
    "- **adam:** Adam (short for Adaptive Moment Estimation) is an optimization algorithm that adjusts the learning rate for each parameter. It's efficient and well-suited for a wide range of problems, especially those with large datasets or many parameters.\n",
    "\n",
    "**Loss Function:**\n",
    " - **categorical_crossentropy** This loss function is used for multi-class classification tasks where the labels are one-hot encoded. It measures the difference between the predicted probability distribution and the actual distribution (one-hot encoded labels).\n",
    "\n",
    "**Metrics:**\n",
    " - **accuracy:** This metric calculates how often the model's predictions match the true labels. Accuracy is a common metric for classification tasks.\n",
    " \n",
    "**Training Data:**\n",
    "\n",
    "- xtrain: The training input data (padded sequences of text).\n",
    "- ytrain: The training target data (one-hot encoded labels).\n",
    "\n",
    "**Epochs:**\n",
    "- **epochs=10:** The number of times the entire training dataset will be passed through the model. Each epoch consists of one complete forward and backward pass of all the training examples.\n",
    "\n",
    "**Batch Size:**\n",
    " - **batch_size=32:** The number of training examples utilized in one forward/backward pass. The model's weights are updated once per batch.\n",
    "\n",
    "**Validation Data:**\n",
    " - **validation_data=(xtest, ytest):** Data used to evaluate the model's performance during training, but not used for updating the model's weights. This helps to monitor overfitting and gives an indication of how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4802e",
   "metadata": {},
   "source": [
    "**Now let’s take a sentence as an input text and see how the model performs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490629b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are places i feel so good visiting\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "['joy']\n"
     ]
    }
   ],
   "source": [
    "input_text = str(input())\n",
    "\n",
    "# Preprocess the input text\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length)\n",
    "prediction = model.predict(padded_input_sequence)\n",
    "predicted_label = label_encoder.inverse_transform([np.argmax(prediction[0])])\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e2e86",
   "metadata": {},
   "source": [
    "This is how you can leverage Machine Learning for text emotion classification using Python. Text emotion classification involves assigning an emotion to a piece of text by understanding its context and underlying sentiment. A practical example of this is the iPhone keyboard, which suggests the most relevant emoji based on the text input. I hope you found this article on Text Emotion Classification with Machine Learning using Python informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93334382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
