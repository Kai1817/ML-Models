{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2356f3",
   "metadata": {},
   "source": [
    "## Next Word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea96b71",
   "metadata": {},
   "source": [
    "- Predicting the most likely word or phrase to appear next in a sentence or text is known as \"**next word prediction.**\" \n",
    "- It's similar to an application's built-in function that predicts the next word you'll write or say.\n",
    "- Applications such as messaging apps, search engines, virtual assistants, and smartphone autocorrect functions use the Next Word Prediction Models. \n",
    "- Thus, this post is for you if you wish to learn how to create a Next Word Prediction Model. \n",
    "- I'll walk you through creating a Next Word Prediction Model with Python and Deep Learning in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71931878",
   "metadata": {},
   "source": [
    "### What is the Next Word Prediction Model & How to Build it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f748f",
   "metadata": {},
   "source": [
    "- Next word prediction, a task in Machine Learning language modeling, seeks to anticipate the word or sequence of words most likely to follow a provided input context. \n",
    "- This process relies on statistical patterns and linguistic structures to make precise predictions according to the given context.\n",
    "- Next Word Prediction models find utility in diverse sectors. \n",
    "- For instance, in mobile messaging, they offer word suggestions to hasten typing. \n",
    "- Likewise, search engines propose search terms as users type. \n",
    "- This technology expedites communication and search processes by anticipating user inputs accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b639c77",
   "metadata": {},
   "source": [
    "**To construct a Next Word Prediction model**\n",
    "\n",
    "- begin by gathering a varied dataset of textual materials,\n",
    "- cleanse and tokenize the data for preprocessing,\n",
    "- organize the data into input-output pairs,\n",
    "- create word embeddings as part of feature engineering,\n",
    "- choose a suitable model such as LSTM or GPT,\n",
    "- train the model on the dataset while fine-tuning hyperparameters,\n",
    "- enhance the model by exploring diverse methods and structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b986bd",
   "metadata": {},
   "source": [
    "- **This cyclical approach enables businesses to create precise and effective Next Word Prediction models adaptable to different contexts.** \n",
    "- **Initiating the construction of a Next Word Prediction model involves gathering textual data, essentially forming the vocabulary for the model.**\n",
    "- **For instance, the input from a smartphone keyboard serves as the vocabulary for its predictive text feature.** \n",
    "- **Similarly, I've identified an optimal dataset sourced from a Sherlock Holmes book text for this purpose.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8342d8",
   "metadata": {},
   "source": [
    "### Next Word Prediction Model using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35ae22",
   "metadata": {},
   "source": [
    "- I hope you now know what a Next Word Prediction model is. \n",
    "- In this section, I’ll take you through how to build a Next Word Prediction model using Python and Deep Learning. \n",
    "- So, let’s start this task by importing the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb53526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Read the text file\n",
    "with open('C:/Users/asus/OneDrive/Desktop/ML_Datasets/project/More_Projects/sherlock-holm.es_stories_plain-text_advs.txt', \n",
    "          'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f6759",
   "metadata": {},
   "source": [
    "**Now let’s tokenize the text to create a sequence of words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f5ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8a67d",
   "metadata": {},
   "source": [
    "- In the provided code, the text undergoes tokenization, separating it into individual tokens or words.\n",
    "- A 'Tokenizer' object is instantiated to manage this process. The 'fit_on_texts' method of the tokenizer is invoked with the 'text' as its argument. \n",
    "- This method examines the text, creating a lexicon of unique words and assigning each a numeric index. \n",
    "- Subsequently, the variable 'total_words' is set to the length of the word index plus one, representing the total count of unique words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c9ff9",
   "metadata": {},
   "source": [
    "#### Now let’s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816cff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd62706",
   "metadata": {},
   "source": [
    "In the provided code snippet, the text is segmented into lines using the newline character ('\\n') as a separator.\n",
    "- Each line is then converted into a sequence of numeric tokens using the 'texts_to_sequences' method of the tokenizer, which utilizes the previously established vocabulary. \n",
    "- These token sequences are processed iteratively using a for loop. During each iteration, a subset of tokens, forming an n-gram sequence, is extracted from the beginning of the token list up to the current index 'i'. This n-gram sequence serves as the input context, with the final token representing the target or predicted word. \n",
    "- These input-output sequences are aggregated into the 'input_sequences' list, resulting in multiple training instances for the next word prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287861d",
   "metadata": {},
   "source": [
    "#### Now let’s pad the input sequences to have equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043e3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b567e",
   "metadata": {},
   "source": [
    "In the provided code snippet, the input sequences undergo padding to standardize their lengths. \n",
    "- The variable 'max_sequence_len' is determined by finding the longest sequence among all input sequences. \n",
    "- Using the 'pad_sequences' function, the input sequences are either padded or truncated to match this maximum length. \n",
    "- This function operates on the 'input_sequences' list, specifying the maximum length as 'max_sequence_len' and instructing padding to be added at the beginning of each sequence ('padding=pre'). Subsequently, the input sequences are converted into a numpy array for streamlined processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22c10e",
   "metadata": {},
   "source": [
    "#### Now let’s split the sequences into input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946a8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee5eb1",
   "metadata": {},
   "source": [
    "- In the provided code, the input sequences are partitioned into two arrays labeled as 'X' and 'y', serving as the input and output for training the subsequent word prediction model. \n",
    "- The 'X' array comprises all rows from the 'input_sequences' array, excluding the last column, signifying the input context. Conversely, the 'y' array consists of the values solely from the last column of the 'input_sequences' array, denoting the target or anticipated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959f8f5",
   "metadata": {},
   "source": [
    "#### Now let’s convert the output to one-hot encode vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f709c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c404c8",
   "metadata": {},
   "source": [
    "In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7721e",
   "metadata": {},
   "source": [
    "Now let’s build a neural network architecture to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea235655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">820,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8200</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,238,200</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m820,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m150,600\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8200\u001b[0m)                │       \u001b[38;5;34m1,238,200\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,208,800</span> (8.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,208,800\u001b[0m (8.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=(FutureWarning, DeprecationWarning))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_shape=(max_sequence_len-1,)))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecd7b1",
   "metadata": {},
   "source": [
    "- The provided code outlines the architecture of the next word prediction model. \n",
    "- It employs a 'Sequential' model, representing a linear stack of layers. \n",
    "- The first layer, 'Embedding', transforms input sequences into fixed-size dense vectors. \n",
    "- It requires parameters including 'total_words' for the vocabulary size, '100' for embedding dimensionality, and 'input_length' for sequence length. \n",
    "- Following this, an 'LSTM' layer is added with 150 units to capture sequential patterns. \n",
    "- Finally, a 'Dense' layer with 'total_words' units and a 'softmax' activation function generates output predictions by assigning probabilities to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe07381",
   "metadata": {},
   "source": [
    "Now let’s compile and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaddcc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 28ms/step - accuracy: 0.0600 - loss: 6.5601\n",
      "Epoch 2/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 30ms/step - accuracy: 0.1168 - loss: 5.5713\n",
      "Epoch 3/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 26ms/step - accuracy: 0.1446 - loss: 5.1301\n",
      "Epoch 4/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - accuracy: 0.1630 - loss: 4.7880\n",
      "Epoch 5/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 25ms/step - accuracy: 0.1850 - loss: 4.4373\n",
      "Epoch 6/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 25ms/step - accuracy: 0.2022 - loss: 4.1590\n",
      "Epoch 7/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 30ms/step - accuracy: 0.2326 - loss: 3.8742\n",
      "Epoch 8/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 29ms/step - accuracy: 0.2630 - loss: 3.6026\n",
      "Epoch 9/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 35ms/step - accuracy: 0.3016 - loss: 3.3485\n",
      "Epoch 10/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 33ms/step - accuracy: 0.3366 - loss: 3.1317\n",
      "Epoch 11/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 33ms/step - accuracy: 0.3750 - loss: 2.9149\n",
      "Epoch 12/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 36ms/step - accuracy: 0.4119 - loss: 2.7175\n",
      "Epoch 13/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 31ms/step - accuracy: 0.4414 - loss: 2.5521\n",
      "Epoch 14/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.4739 - loss: 2.3865\n",
      "Epoch 15/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 25ms/step - accuracy: 0.5078 - loss: 2.2276\n",
      "Epoch 16/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 27ms/step - accuracy: 0.5390 - loss: 2.0792\n",
      "Epoch 17/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 28ms/step - accuracy: 0.5623 - loss: 1.9666\n",
      "Epoch 18/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - accuracy: 0.5861 - loss: 1.8514\n",
      "Epoch 19/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 26ms/step - accuracy: 0.6115 - loss: 1.7410\n",
      "Epoch 20/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 26ms/step - accuracy: 0.6345 - loss: 1.6431\n",
      "Epoch 21/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 26ms/step - accuracy: 0.6506 - loss: 1.5572\n",
      "Epoch 22/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.6702 - loss: 1.4667\n",
      "Epoch 23/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - accuracy: 0.6861 - loss: 1.3893\n",
      "Epoch 24/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7022 - loss: 1.3221\n",
      "Epoch 25/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7154 - loss: 1.2603\n",
      "Epoch 26/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7273 - loss: 1.2090\n",
      "Epoch 27/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 27ms/step - accuracy: 0.7407 - loss: 1.1465\n",
      "Epoch 28/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7527 - loss: 1.1009\n",
      "Epoch 29/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 27ms/step - accuracy: 0.7604 - loss: 1.0633\n",
      "Epoch 30/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 27ms/step - accuracy: 0.7717 - loss: 1.0190\n",
      "Epoch 31/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7800 - loss: 0.9745\n",
      "Epoch 32/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7865 - loss: 0.9484\n",
      "Epoch 33/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 27ms/step - accuracy: 0.7913 - loss: 0.9207\n",
      "Epoch 34/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 33ms/step - accuracy: 0.8018 - loss: 0.8792\n",
      "Epoch 35/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 28ms/step - accuracy: 0.8060 - loss: 0.8525\n",
      "Epoch 36/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.8119 - loss: 0.8282\n",
      "Epoch 37/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 27ms/step - accuracy: 0.8179 - loss: 0.7992\n",
      "Epoch 38/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 26ms/step - accuracy: 0.8216 - loss: 0.7792\n",
      "Epoch 39/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 25ms/step - accuracy: 0.8259 - loss: 0.7606\n",
      "Epoch 40/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.8286 - loss: 0.7417\n",
      "Epoch 41/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 26ms/step - accuracy: 0.8332 - loss: 0.7230\n",
      "Epoch 42/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 25ms/step - accuracy: 0.8367 - loss: 0.7101\n",
      "Epoch 43/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 25ms/step - accuracy: 0.8405 - loss: 0.6874\n",
      "Epoch 44/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 26ms/step - accuracy: 0.8419 - loss: 0.6789\n",
      "Epoch 45/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 25ms/step - accuracy: 0.8425 - loss: 0.6706\n",
      "Epoch 46/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 26ms/step - accuracy: 0.8461 - loss: 0.6488\n",
      "Epoch 47/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 26ms/step - accuracy: 0.8496 - loss: 0.6434\n",
      "Epoch 48/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 26ms/step - accuracy: 0.8524 - loss: 0.6292\n",
      "Epoch 49/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.8531 - loss: 0.6168\n",
      "Epoch 50/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 26ms/step - accuracy: 0.8554 - loss: 0.6096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1da7fde0650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c93fc2",
   "metadata": {},
   "source": [
    "- The provided code compiles and trains the model.\n",
    "- The 'compile' method configures it for training, setting 'loss' to 'categorical_crossentropy' for multi-class classification and 'optimizer' to 'adam' for adaptive learning rate. \n",
    "- 'Metrics' is set to 'accuracy' for monitoring. \n",
    "- The 'fit' method trains the model on input sequences 'X' and output 'y', with 'epochs' specifying iteration times. 'Verbose' is set to '1' for display during training. \n",
    "- Upon completion, the model can be used for generating next word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b582da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey there\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "hey there is one thing\n"
     ]
    }
   ],
   "source": [
    "seed_text = input()\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4be355",
   "metadata": {},
   "source": [
    "The provided code generates predictions for the next words based on a given seed text. \n",
    "- 'seed_text' stores the initial text, while 'next_words' determines how many predictions to generate. \n",
    "- Inside the loop, 'seed_text' is converted into token sequences using the tokenizer, then padded to match the maximum sequence length. \n",
    "- The model predicts the next word using the 'predict' method, selecting the word with the highest probability score via 'np.argmax'. \n",
    "- This process repeats for the specified 'next_words'. \n",
    "- Finally, 'seed_text' is printed, displaying the initial text followed by the generated predictions. \n",
    "- This demonstrates the construction of a Next Word Prediction model using Deep Learning and Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab90ee",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ac784",
   "metadata": {},
   "source": [
    "**Next word prediction, a task in Machine Learning, seeks to forecast the most likely word or word sequence following a provided input context. It leverages statistical patterns and linguistic structures to make precise predictions based on the context. I trust you found this article informative on constructing a Next Word Prediction model with Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1081cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
